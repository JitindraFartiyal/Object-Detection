{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JitindraFartiyal/Object-Detection/blob/object-detection-v1/Yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzYgLQahHNYX",
        "colab_type": "text"
      },
      "source": [
        "Connecting to Google drive to upload dataset. This step is only required if you are using Google Colab and uploading dataset from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6KmakpRJu0",
        "colab_type": "code",
        "outputId": "58e7c4e9-ac2b-4165-f6d0-fdea399a6970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCivLdScKWHU",
        "colab_type": "text"
      },
      "source": [
        "Importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxCUs78JHTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from comet_ml import Experiment\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import pdb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from collections import OrderedDict \n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, datasets, utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHecGo64HXlm",
        "colab_type": "text"
      },
      "source": [
        "We need to convert class ['Car','Cyclist'....] in the label file into an integer. As, we are not using label file into our model, we need not to use one hot encoding or other encoding techniques. We are simply converting it for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyzx2ROpIMcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_encoding(label):\n",
        "   \n",
        "  for i in range(label.shape[0]):\n",
        "    if label.iloc[i,0] == 'Car':\n",
        "      label.iloc[i,0] = 1\n",
        "    elif label.iloc[i,0] == 'Cyclist':\n",
        "      label.iloc[i,0] = 2\n",
        "    elif label.iloc[i, 0] == 'DontCare':\n",
        "      label.iloc[i, 0] = 3\n",
        "    elif label.iloc[i,0] == 'Misc':\n",
        "      label.iloc[i,0] = 4\n",
        "    elif label.iloc[i,0] == 'Pedestrian':\n",
        "      label.iloc[i,0] = 5\n",
        "    elif label.iloc[i,0] == 'Person_sitting':\n",
        "      label.iloc[i,0] = 6\n",
        "    elif label.iloc[i,0] == 'Tram':\n",
        "      label.iloc[i,0] = 7\n",
        "    elif label.iloc[i,0] == 'Truck':\n",
        "      label.iloc[i,0] = 8\n",
        "    elif label.iloc[i,0] == 'Van':\n",
        "      label.iloc[i,0] = 9 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCrcuTHIWGC",
        "colab_type": "text"
      },
      "source": [
        "Kitti Dataset has different format for label file as compared to the YOLO format for label file. We need to convert format of our Kitti Dataset label file into format of YOLO label file.                                                       \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Note : We are rescaling coordinates of our bounding box into output image size which is [225 X 225] and not the input image size which is [270 X 270], as we need to compare the labels with the ouput of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onG5FIbgIeYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_label(label, number_of_classes):\n",
        "  \n",
        "  # In case of Bounding boxes, coordinate system doesnot start from bottom-left as we see normally in our mathematics, instead it starts from top-left corner\n",
        "  top_left_x = label[:,1]\n",
        "  top_left_y = label[:,2]\n",
        "  bottom_right_x = label[:,3]\n",
        "  bottom_right_y = label[:,4]\n",
        "\n",
        "  height = bottom_right_y - top_left_y\n",
        "  width = bottom_right_x - top_left_x\n",
        "  center_x = top_left_x + width/2\n",
        "  center_y = top_left_y + height/2\n",
        "\n",
        "  # Reducing the scale [1242 X 375] of the coordinates of bounding box in the label file into output image scale [225 X 225]. \n",
        "  # We need to do this, so that at training and testing, we can compute loss easily, if all are in the same scale. \n",
        "  label[:,1] = (center_x/1242)*225\n",
        "  label[:,2] = (center_y/375)*225\n",
        "  label[:,3] = (height /375)*225\n",
        "  label[:,4] = (width/1242)*225\n",
        "\n",
        "  # Adding classes probabilites columns\n",
        "  target = np.zeros((label.shape[0],label.shape[1] + number_of_classes)) \n",
        "  target[:,0:5] = label\n",
        "  \n",
        "  for i in range(0,label.shape[0]):\n",
        "    if(target[i,0:1] == 1): # Prob_Class(Car) = 1 and rest 0, if Car is detected \n",
        "      target[i,5:6] = 1\n",
        "    elif(target[i,0:1] == 2): # Prob_Class(Cyclist) = 1 and rest 0, if Cyclist is detected\n",
        "      target[i,6:7] = 1\n",
        "    elif(target[i,0:1] == 3): # Prob_Class(DontCare) = 1 and rest 0, if DontCare is detected\n",
        "      target[i,7:8] = 1  \n",
        "    elif(target[i,0:1] == 4): # Prob_Class(Misc) = 1 and rest 0, if Misc is detected\n",
        "      target[i,8:9] = 1\n",
        "    elif(target[i,0:1] == 5): # Prob_Class(Pedestrian) = 1 and rest 0, if Pedestrian is detected\n",
        "      target[i,9:10] = 1\n",
        "    elif(target[i,0:1] == 6): # Prob_Class(Person_sitting) = 1 and rest 0, if Person_sitting is detected \n",
        "      target[i,10:11] = 1  \n",
        "    elif(target[i,0:1] == 7): # Prob_Class(Tram) = 1 and rest 0, if Tram is detected\n",
        "      target[i,11:12] = 1\n",
        "    elif(target[i,0:1] == 8): # Prob_Class(Truck) = 1 and rest 0, if Truck is detected \n",
        "      target[i,12:13] = 1\n",
        "    elif(target[i,0:1] == 9): # Prob_Class(Van) = 1 and rest 0, if Van is detected \n",
        "      target[i,13:14] = 1\n",
        "  return target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xShT0UeFkCyi",
        "colab_type": "text"
      },
      "source": [
        "We need to preprocess the data. It means to keep the data i.e the input to our Convolutional Neural Network (CNN model) into an uniform form.\n",
        "Here, our input will be a dictionary of image and its label. Only, the images will be the input for our CNN model and label will be used for calculating loss. Largely, we only need to preprocess the data which we input in our CNN model with resizing, normalizing, mean subtraction etc., but one common preprocessing on both image and label is needed to convert it into a tensor for further calculation.\n",
        "Here, for image we are only resizing it into [270 X 270] and Mean Subtraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW-2inMnkFQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Resize(object):\n",
        "\n",
        "  def __init__(self, input_image_size):\n",
        "    # Input image size is the size of the image that we are putting it into our CNN Model. In this case, it is [270 X 270]\n",
        "    self.input_image_size = (input_image_size,input_image_size)\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = transform.resize(image,self.input_image_size,preserve_range=True)\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "class ToTensor(object):\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = image.transpose((2, 0, 1)) # Converting the image form from (H X W X C) into (C X H X W)\n",
        "\n",
        "    # If we donot use float() at end, by default, torch.from_numpy() will convert our input of our CNN model into a Float64 type\n",
        "    # We have to convert our CNN model type also in Float64 i.e Double or else it will throw error. \n",
        "    # By default, CNN model type is Float16, so better to convert the input into Float16 type here only\n",
        "    return {'image' : torch.from_numpy(image).float(),\n",
        "            'label' : torch.from_numpy(label).float()}\n",
        "\n",
        "class MeanSubtraction(object):\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = np.absolute(image - np.mean(image)) # Subtracting Mean from all the pixels of the image of all channels. Refer NumPy mannual to know more.\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "# Our Dataset has different lengths data in our label file, so when stacking into a single batch during training, it throw error because of\n",
        "# variable dimensions. One of the solution is to pad the label file with an arbitrary number.  \n",
        "class BatchPadding(object):\n",
        "\n",
        "  def __init__(self, pad):\n",
        "    self.pad = pad\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    batched_label = np.zeros((self.pad,label.shape[1]))\n",
        "    batched_label[0:label.shape[0],:] = label\n",
        "\n",
        "    return {'image' : image, 'label' : batched_label}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpTW24lZIr0w",
        "colab_type": "text"
      },
      "source": [
        "Here, we are defining a class for our dataset. For our problem of Object Detection for Self Driving Cars, we are using KittiDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nlWbN5nIxZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KittiDataset(Dataset):\n",
        "\n",
        "    def __init__(self, labels_dir, images_dir, number_of_classes, transform=None):\n",
        "      \n",
        "      self.labels_dir = labels_dir\n",
        "      self.images_dir = images_dir\n",
        "      self.number_of_classes = number_of_classes\n",
        "      self.transform = transform\n",
        "\n",
        "      self.labels_dict = {}\n",
        "      self.filename = []\n",
        "      self.__init__dataset()\n",
        "\n",
        "    def __init__dataset(self):\n",
        "      \n",
        "      print('..........Initializing Dataset..........')\n",
        "      \n",
        "      index = 0\n",
        "      for file in os.listdir(self.labels_dir):\n",
        "\n",
        "        print('Reading label file : ' + file + '...')\n",
        "        \n",
        "        label_path = self.labels_dir + '/' + file\n",
        "        label = pd.read_csv(filepath_or_buffer=label_path, sep=' ', header=None, index_col=False)\n",
        "        \n",
        "        # Taking out relevant features out from the label dataframe\n",
        "        label = label.iloc[:,[0,4,5,6,7]] \n",
        "        label.columns = ['Class','TopLeftX','TopLeftY','BottomRightX','BottomRightY'] \n",
        "    \n",
        "        # Class Encoding\n",
        "        # Car=1, Cyclist=2, DontCare=3, Misc=4, Pedestrian=5, Person_sitting=6, Tram=7, Truck=8, Van=9 \n",
        "        class_encoding(label)\n",
        "\n",
        "        self.labels_dict[index] = label\n",
        "        self.filename.append(file[0:6])\n",
        "        index = index + 1\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels_dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image_path = self.images_dir + '/' + self.filename[index] + '.png'\n",
        "      image = io.imread(image_path)\n",
        "      \n",
        "      label = self.labels_dict[index]\n",
        "      label = label.to_numpy(dtype = np.float16) \n",
        "      \n",
        "      # Convert the label into YOLO format (class, center_x, center_y, height, width, class_prob1 ..... class_probn)\n",
        "      target = transform_label(label, self.number_of_classes)\n",
        "\n",
        "      data_sample = {'image' : image, 'label' : target}\n",
        "      \n",
        "      if self.transform:\n",
        "        data_sample = self.transform(data_sample)\n",
        "        \n",
        "      return data_sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvQc2zSI5QT",
        "colab_type": "text"
      },
      "source": [
        "After creating the datatset class, we now need to create our CNN model class, where we define our CNN layers and forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm2SIYEI8TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, gridX, gridY, grids_values):\n",
        "      \n",
        "        super(Net, self).__init__()\n",
        "        print('..........Initializing Convolutional Neural Network..........')\n",
        "      \n",
        "        # Fully Connected layer settings - Includes the size of Fully Connected layer\n",
        "        self.gridX = gridX\n",
        "        self.gridY = gridY\n",
        "        self.grids_values = grids_values\n",
        "        \n",
        "        # Initialization of Convolutional layers, Batch Normalization layers and Dropout layers\n",
        "        self.cnnlayer1 = nn.Sequential(OrderedDict([\n",
        "          ('conv1', nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu1', nn.ReLU(True)),\n",
        "          ('batch_norm_conv1', nn.BatchNorm2d(num_features=64))\n",
        "         ]))\n",
        "        \n",
        "        self.cnnlayer2 = nn.Sequential(OrderedDict([\n",
        "          ('conv2', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu2', nn.ReLU(True)),\n",
        "          ('batch_norm_conv2', nn.BatchNorm2d(num_features=128))\n",
        "         ]))\n",
        "        \n",
        "        self.cnnlayer3 = nn.Sequential(OrderedDict([\n",
        "          ('conv3', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu3', nn.ReLU(True)),\n",
        "          ('batch_norm_conv3', nn.BatchNorm2d(num_features=256))\n",
        "         ]))\n",
        "        \n",
        "        self.cnnlayer4 = nn.Sequential(OrderedDict([\n",
        "          ('conv4', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu4', nn.ReLU(True)),\n",
        "          ('batch_norm_conv4', nn.BatchNorm2d(num_features=512)),\n",
        "          ('dropout_conv5', nn.Dropout2d(0.05))\n",
        "         ]))\n",
        "\n",
        "        self.cnnlayer5 = nn.Sequential(OrderedDict([\n",
        "          ('conv5', nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu5', nn.ReLU(True)),\n",
        "          ('batch_norm_conv5', nn.BatchNorm2d(num_features=256)),\n",
        "          ('max_pool_conv5', nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "         ]))\n",
        "        \n",
        "        self.cnnlayer6 = nn.Sequential(OrderedDict([\n",
        "          ('conv6', nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu6', nn.ReLU(True)),\n",
        "          ('batch_norm_conv6', nn.BatchNorm2d(num_features=128)),\n",
        "          ('max_pool_conv6', nn.MaxPool2d(kernel_size=3, stride=3))\n",
        "         ]))\n",
        "        \n",
        "        self.cnnlayer7 = nn.Sequential(OrderedDict([\n",
        "          ('conv7', nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=0, bias=True)),\n",
        "          ('relu7', nn.ReLU(True)),\n",
        "          ('batch_norm_conv7', nn.BatchNorm2d(num_features=64)),\n",
        "          ('max_pool7', nn.MaxPool2d(kernel_size=3, stride=3))\n",
        "         ]))\n",
        "        \n",
        "        self.fc1 = nn.Linear(self.gridX*self.gridY*64, self.gridX*self.gridY*32, bias=True)\n",
        "        self.batch_norm_fc1 = nn.BatchNorm1d(num_features=self.gridX*self.gridY*32)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc2 = nn.Linear(self.gridX*self.gridY*32, self.gridX*self.gridY*self.grids_values, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.cnnlayer1(x)\n",
        "        x = self.cnnlayer2(x)\n",
        "        x = self.cnnlayer3(x)\n",
        "        x = self.cnnlayer4(x)\n",
        "\n",
        "        x = self.cnnlayer5(x)\n",
        "        x = self.cnnlayer6(x)\n",
        "        x = self.cnnlayer7(x)\n",
        "\n",
        "        x = x.view(-1,self.gridX*self.gridY*64)\n",
        "        x = self.batch_norm_fc1(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(self.dropout2(x))\n",
        "        x = F.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHSSYHVuJDIK",
        "colab_type": "text"
      },
      "source": [
        "After prediction, we will get many bounding boxes for a single class. To eliminate that, we need an algorithm to find which bounding box matches the ground truth bounding box by how much. We call this algorithm Intersection of Union (IOU).\n",
        "\n",
        "\n",
        "```\n",
        "IOU = (area of intersection) / (area of bounding box1 + area of bounding box2 - area of intersection) \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x-gK3JJJGq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_IOU(b1X, b1Y, b2X, b2Y, b3X, b3Y, b4X, b4Y):\n",
        "\n",
        "  # b1X, b1Y, b2X, b2Y corresponds to topleft and bottom right coordinates of bounding box1 \n",
        "  # b3X, b3Y, b4X, b4Y corresponds to topleft and bottom right coordinates of bounding box2\n",
        "  xA = max(b1X,b3X)\n",
        "  yA = max(b1Y,b3Y)\n",
        "  xB = min(b2X,b4X)\n",
        "  yB = min(b2Y,b4Y)\n",
        "\n",
        "  area_intersection = (xB-xA+1) * (yB-yA+1)\n",
        "  area_of_boundingbox1 = (b2X-b1X+1) * (b2Y-b1Y+1)\n",
        "  area_of_boundingbox2 = (b4X-b3X+1) * (b4Y-b4Y+1)\n",
        "\n",
        "  iou = area_intersection/(area_of_boundingbox1 + area_of_boundingbox2 - area_intersection)\n",
        "  return iou\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAsc1wGM8urT",
        "colab_type": "text"
      },
      "source": [
        "After calculating IOU of all the bouding boxes, we need to return the bounding box whose IOU is the highest. \n",
        "\n",
        "\n",
        "---\n",
        "Remember the coordinates of the bounding box are scaled i.e x,y are offsets with respect to grid and h,w are scaled between 0 and 1 with respect to image height and width \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCijFzM48uBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, grids):\n",
        "  max_iou = 0\n",
        "  max_iou_index = 0\n",
        "  \n",
        "  # Logic to get the top-left coordinates of grid cell\n",
        "  x_offset = int(grid_cell / grid_offset)\n",
        "  y_offset = int(grid_cell % grid_offset)\n",
        "  grid_x_offset = (x_offset*grid_offset) \n",
        "  grid_y_offset = (y_offset*grid_offset)   \n",
        "  \n",
        "  for number_of_bounding_box in range(0,bounding_boxes):\n",
        "\n",
        "    # We will first calculate the center,height,width coordinates and then find topleft and bottomright coordinates of the bounding box\n",
        "    predicted_center_x = (predicted_grid_output[(number_of_bounding_box*5) + 1].item()*grid_offset) + grid_x_offset\n",
        "    predicted_center_y = (predicted_grid_output[(number_of_bounding_box*5) + 2].item()*grid_offset) + grid_y_offset\n",
        "    predicted_height = predicted_grid_output[(number_of_bounding_box*5) + 3].item()*grids\n",
        "    predicted_width= predicted_grid_output[(number_of_bounding_box*5) + 4].item()*grids\n",
        "\n",
        "    predicted_topLeftX = predicted_center_x - predicted_width/2\n",
        "    predicted_topLeftY = predicted_center_y - predicted_height/2\n",
        "    predicted_bottomRightX = predicted_center_x + predicted_width/2\n",
        "    predicted_bottomRightY = predicted_center_y + predicted_height/2\n",
        "\n",
        "    ground_truth_center_x = ground_truth_grid_output[1].item()\n",
        "    ground_truth_center_y = ground_truth_grid_output[2].item()\n",
        "    ground_truth_height = ground_truth_grid_output[3].item()\n",
        "    ground_truth_width = ground_truth_grid_output[4].item()\n",
        "\n",
        "    ground_truth_topLeftX = ground_truth_center_x - ground_truth_width/2\n",
        "    ground_truth_topLeftY = ground_truth_center_y - ground_truth_height/2\n",
        "    ground_truth_bottomRightX = ground_truth_center_x + ground_truth_width/2\n",
        "    ground_truth_bottomRightY = ground_truth_center_y + ground_truth_height/2\n",
        "\n",
        "    iou = calculate_IOU(predicted_topLeftX,predicted_topLeftY,predicted_bottomRightX,predicted_bottomRightY,\n",
        "                        ground_truth_topLeftX,ground_truth_topLeftY,ground_truth_bottomRightX,ground_truth_bottomRightY)\n",
        "\n",
        "    if(iou > max_iou):\n",
        "      max_iou = iou\n",
        "      max_iou_index = number_of_bounding_box\n",
        "\n",
        "  return max_iou_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZTDH6dJWlD",
        "colab_type": "text"
      },
      "source": [
        "Now, we will calculate the Loss function. It comprises of three losses :\n",
        "\n",
        "*   Classification Loss : if object is detected, the mean squared error loss of class probabilites\n",
        "*   Localization Loss : if object is detected, the mean squared error loss of coordinates of bounding box\n",
        "*   Confidence Loss : the mean squared error loss of box confidence, when object is detected and when it is not\n",
        "\n",
        "In the end, we will mulitply our loss with lambda_coord and lambda_noobject which regularize the imbalance and reduce the effect of background noise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h866UlMIy9la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def yolo_loss(batched_output, batched_label, grids, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "  total_loss = 0\n",
        "  grid_offset = np.sqrt(grids) # Image is divide in 15 X 15, so one grid spans 1/15 from center in range of (0-1)\n",
        "\n",
        "  for batch_number in range(0,batched_output.size()[0]):\n",
        "    classification_loss = 0\n",
        "    localization_loss_centerpoint = 0\n",
        "    localization_loss_aspect_ratio = 0\n",
        "    confidence_loss_object = 0\n",
        "    confidence_loss_noobject = 0\n",
        "\n",
        "    for grid_cell in range(0,batched_output.size()[1]):\n",
        "      \n",
        "      predicted_grid_output = batched_output[batch_number,grid_cell,:]\n",
        "\n",
        "      # Logic to get the center coordinates of grid cell\n",
        "      x_offset = int(grid_cell / grid_offset)\n",
        "      y_offset = int(grid_cell % grid_offset)\n",
        "      grid_cell_center_x = (x_offset*grid_offset) + (grid_offset/2)\n",
        "      grid_cell_center_y = (y_offset*grid_offset) + (grid_offset/2)\n",
        "\n",
        "      object_present = 0\n",
        "      ground_truth_grid_output = torch.Tensor()\n",
        "       \n",
        "      for index in range(0,batched_label.size()[1]):\n",
        "        ground_truth_grid_output = batched_label[batch_number,index,:]\n",
        "        \n",
        "        if (ground_truth_grid_output.sum() == 0):\n",
        "          break\n",
        "        \n",
        "        ground_truth_center_x = ground_truth_grid_output[1].item()\n",
        "        ground_truth_center_y = ground_truth_grid_output[2].item()\n",
        "        ground_truth_height = ground_truth_grid_output[3].item()\n",
        "        ground_truth_width = ground_truth_grid_output[4].item()\n",
        "\n",
        "        # Finding whether grid detects an object or not\n",
        "        if(ground_truth_center_x <= (grid_cell_center_x+(grid_offset/2)) and ground_truth_center_x >= (grid_cell_center_x-(grid_offset/2))\n",
        "            and ground_truth_center_y <= (grid_cell_center_y+(grid_offset/2)) and ground_truth_center_y >= (grid_cell_center_y-(grid_offset/2))):\n",
        "          object_present = 1\n",
        "          break\n",
        "            \n",
        "      # Calculating classification loss\n",
        "      if(object_present == 1):\n",
        "\n",
        "        partial_classification_loss = 0\n",
        "        for object_class in range(0,classes):\n",
        "          partial_classification_loss += (predicted_grid_output[5*bounding_boxes+object_class].item() - ground_truth_grid_output[5+object_class].item()) ** 2 \n",
        "      \n",
        "        classification_loss += partial_classification_loss\n",
        "\n",
        "        # Calculating which bounding box has highest IOU with ground truth bounding box\n",
        "        highest_iou_bbox_index = find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, grids)\n",
        "\n",
        "        # Calculating localization loss of center points and aspect ratio\n",
        "        grid_cell_topleftX  = grid_cell_center_x-(grid_offset/2)\n",
        "        grid_cell_topleftY  = grid_cell_center_y-(grid_offset/2)\n",
        "\n",
        "        predicted_center_x = (predicted_grid_output[(highest_iou_bbox_index*5)+1].item())*grid_offset + grid_cell_topleftX\n",
        "        predicted_center_y = (predicted_grid_output[(highest_iou_bbox_index*5)+2].item())*grid_offset + grid_cell_topleftY\n",
        "        predicted_height = predicted_grid_output[(highest_iou_bbox_index*5)+3].item()*grids\n",
        "        predicted_width = predicted_grid_output[(highest_iou_bbox_index*5)+4].item()*grids\n",
        "     \n",
        "        localization_loss_centerpoint += (predicted_center_x-ground_truth_grid_output[1].item())**2 + (predicted_center_y-ground_truth_grid_output[2].item())**2\n",
        "        localization_loss_aspect_ratio += (math.sqrt(predicted_height)-math.sqrt(ground_truth_grid_output[3].item()))**2 + (math.sqrt(predicted_width)-math.sqrt(ground_truth_grid_output[4].item()))**2\n",
        "\n",
        "        # Calculating Confidence loss, if object detected\n",
        "        confidence_loss_object += (predicted_grid_output[highest_iou_bbox_index*5].item()-ground_truth_grid_output[0].item())**2\n",
        "              \n",
        "      # Calculating Confidence loss, if object not detected\n",
        "      else:\n",
        "        for number_of_bounding_box in range(0,bounding_boxes):\n",
        "          confidence_loss_noobject += (predicted_grid_output[number_of_bounding_box*5].item())**2\n",
        "\n",
        "    total_loss += classification_loss + lambda_coord*localization_loss_centerpoint + lambda_coord*localization_loss_aspect_ratio + confidence_loss_object + lambda_noobject*confidence_loss_noobject \n",
        "  \n",
        "  return torch.tensor([total_loss/batched_output.size()[0]], requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9vSE7fmJeRR",
        "colab_type": "text"
      },
      "source": [
        "Our Convolutional Neural newtork is defined, dataset is defined, loss function is defined. Now, we will train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbFltpQ6JgdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, training_dataloader, batch_size, grids, grids_values, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "  \n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training \n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode\n",
        "  model.train()\n",
        "  batch_loss = 0\n",
        "  \n",
        "  for batch_index, batched_sample in enumerate(training_dataloader):\n",
        "    \n",
        "    batched_image = batched_sample['image']\n",
        "    batched_label = batched_sample['label']\n",
        "    batched_output = model(batched_image)\n",
        "    batched_output = batched_output.view(batch_size, grids, grids_values) # Convert the output size into [N X GRIDS(225 X 225) X (5 * B + C)]\n",
        "\n",
        "    loss = yolo_loss(batched_output, batched_label, grids, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "    print('Loss for batch_index : {} is {}'.format(batch_index,loss.item()))\n",
        "    batch_loss = batch_loss + loss.item()\n",
        "    \n",
        "    optimizer.zero_grad()    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  return batch_loss/len(training_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOqC1ERJ5xp",
        "colab_type": "text"
      },
      "source": [
        "We have trained our model, now we will validate our model. Validation is required to tune our model parameters and hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA1zdHgdJ8Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model, testing_dataloader, batch_size, object_detected_threshold, box_confidence_threshold, bounding_boxes, grids,grids_values):\n",
        "\n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training \n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode  \n",
        "  model.eval()\n",
        "  grid_offset = np.sqrt(grids) # Image is divide in 15 X 15, so one grid spans 1/15 from center in range of (0-1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_index, batched_validation_sample in enumerate(testing_dataloader):\n",
        "      \n",
        "      print('Validating for batch index : {} .....'.format(batch_index))\n",
        "\n",
        "      validation_image = batched_validation_sample['image']\n",
        "      validation_label = batched_validation_sample['label']\n",
        "      output = model(validation_image)\n",
        "      output = output.view(batch_size, grids,grids_values)\n",
        "\n",
        "      original_image  = validation_image.permute(0,2,3,1) # Converting out image back from [N X H X W] to [H X W X N] size\n",
        "\n",
        "      for batch_number in range(0,batch_size):\n",
        "        original_image = original_image[batch_number,:,:,:]\n",
        "        original_image = original_image.numpy()\n",
        "        original_image = transform.resize(original_image,(375,1242),preserve_range=True)\n",
        "        original_image = np.ascontiguousarray(original_image, dtype=np.uint8)\n",
        "        \n",
        "        for grid_cell in range(0,grids):\n",
        "\n",
        "          grid_output = output[batch_number,grid_cell,:]\n",
        "          \n",
        "          # # Logic to get the center, top-left and bottom-right coordinates of grid cell\n",
        "          x_offset = int(grid_cell / grid_offset)\n",
        "          y_offset = int(grid_cell % grid_offset)\n",
        "        \n",
        "          grid_cell_center_x = (x_offset*grid_offset) + (grid_offset/2)\n",
        "          grid_cell_center_y = (y_offset*grid_offset) + (grid_offset/2)\n",
        "        \n",
        "          grid_cell_topleftX  = grid_cell_center_x-(grid_offset/2)\n",
        "          grid_cell_topleftY  = grid_cell_center_y-(grid_offset/2)\n",
        "        \n",
        "          for number_of_bounding_box in range(0,bounding_boxes):\n",
        "          \n",
        "            if(grid_output[number_of_bounding_box*5] >= object_detected_threshold):\n",
        "            \n",
        "              class_probabilities = grid_output[5*bounding_boxes:]\n",
        "              max_class_probabilites, index = torch.max(class_probabilities,0)\n",
        "\n",
        "              if(max_class_probabilites.item() >= box_confidence_threshold):\n",
        "                accuracy = max_class_probabilites.item()*100\n",
        "                \n",
        "                center_x = grid_output[(number_of_bounding_box*5)+1]*grid_offset + grid_cell_topleftX\n",
        "                center_y = grid_output[(number_of_bounding_box*5)+2]*grid_offset + grid_cell_topleftY\n",
        "                height = grid_output[(number_of_bounding_box*5)+3]*grids\n",
        "                width = grid_output[(number_of_bounding_box*5)+4]*grids\n",
        "\n",
        "                top_left_x = ((center_x - (width/2))/grids)*original_image.shape[1]\n",
        "                top_left_y = ((center_y - (height/2))/grids)*original_image.shape[0]\n",
        "                bottom_right_x = ((center_x + (width/2))/grids)*original_image.shape[1]\n",
        "                bottom_right_y = ((center_y + (height/2))/grids)*original_image.shape[0]\n",
        "\n",
        "                name = ''\n",
        "                if index.item() == 1:\n",
        "                  name = 'Car'\n",
        "                elif index.item() == 2:\n",
        "                  name = 'Cyclist'\n",
        "                elif index.item() == 3:\n",
        "                  name = 'DontCare'\n",
        "                elif index.item() == 4:\n",
        "                  name = 'Misc'\n",
        "                elif index.item() == 5:\n",
        "                  name = 'Pedestrian'\n",
        "                elif index.item() == 6:\n",
        "                  name = 'Person_sitting'\n",
        "                elif index.item() == 7:\n",
        "                  name = 'Tram'\n",
        "                elif index.item() == 8:\n",
        "                  name = 'Truck'\n",
        "                elif index.item() == 9:\n",
        "                  name = 'Van' \n",
        "\n",
        "                  name = name + '     (' + str(accuracy) + ')'               \n",
        "                \n",
        "                original_image = cv2.rectangle(original_image,(top_left_x,top_left_y),(bottom_right_x,bottom_right_y),(0,0,255),3)\n",
        "                cv2.putText(original_image, name, (top_left_x, top_left_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
        "\n",
        "        save_path = r'/content/gdrive/My Drive/kitti_single_mini/validation/results/image' + str(batch_index) + '.jpeg'\n",
        "        cv2.imwrite(save_path,original_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9gLfZyJox3",
        "colab_type": "text"
      },
      "source": [
        "We will finish our program by writing a main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkCwlPQJqWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    print('..........Main Function starts..........')\n",
        "    \n",
        "    # Training Settings\n",
        "    lr = 0.000001 # Hyper parameters\n",
        "    betas = (0.9,0.999) # Hyper parameters\n",
        "    epochs = 5 # Hyper parameters\n",
        "    training_batch_size = 4 # Hyper parameters\n",
        "    \n",
        "    validation_batch_size = 1\n",
        "    object_detected_threshold = 0.5 # Model Parameters\n",
        "    box_confidence_threshold = 0.7 # Model Parameters\n",
        "    grids = 15*15 # Model Parameters\n",
        "    bounding_boxes = 2 # Model Parameters\n",
        "    classes = 9 # Model Parameters\n",
        "    grids_values = (5*bounding_boxes) + classes # Model Parameters\n",
        "    lambda_coord = 2 # Model Parameters\n",
        "    lambda_noobject = 0.5 # Model Parameters\n",
        "    save_model = False\n",
        "    seed = 1\n",
        "    logging = True\n",
        "    steps_completed = 0\n",
        "    last_epoch_loss = 0\n",
        "    number_of_training_data = 0\n",
        "    training_loss = 0\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "\n",
        "    # Comet ML Settings for visualizing loss function and hyper parameters\n",
        "    if(logging):\n",
        "      experiment = Experiment(api_key=\"Vxlozksi1tLwXJlmZYjfQVm7w\", project_name=\"object-detection\", workspace=\"jayfartiyal\")\n",
        "      hyper_params = {\"lr\": lr, \"epochs\": epochs, \"batch_size\":training_batch_size} \n",
        "      experiment.log_parameters(hyper_params)\n",
        "\n",
        "    # Images and labels Directory\n",
        "    labels_dir = r'/content/gdrive/My Drive/kitti_single_mini/training/label_2'\n",
        "    images_dir = r'/content/gdrive/My Drive/kitti_single_mini/training/image_2'\n",
        "    validation_images_dir = r'/content/gdrive/My Drive/kitti_single_mini/validation/image_2'\n",
        "    validation_labels_dir = r'/content/gdrive/My Drive/kitti_single_mini/validation/label_2'\n",
        "    saving_model_path = r'/content/gdrive/My Drive/kitti_single_mini/validation/layer9class2.cnn.pt'\n",
        "\n",
        "\n",
        "    if(images_dir.find('micro') != -1):\n",
        "      number_of_training_data = 100\n",
        "    elif(images_dir.find('mini') != -1):\n",
        "      number_of_training_data = 250\n",
        "    elif(images_dir.find('small') != -1):\n",
        "      number_of_training_data = 500\n",
        "    elif(images_dir.find('medium') != -1):\n",
        "      number_of_training_data = 1000\n",
        "\n",
        "\n",
        "    if(save_model == False):\n",
        "      \n",
        "      #Inititalizing model and optimizer\n",
        "      gridX = int(math.sqrt(grids))\n",
        "      gridY = gridX\n",
        "      model = Net(gridX,gridY,grids_values)\n",
        "      optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
        "      print('..........Convolutional Neural Network model and optimizer has been initialized..........')\n",
        "\n",
        "      # Retrieving model and optimizer states if present \n",
        "      if(os.path.isfile(saving_model_path)):\n",
        "\n",
        "        print('.....Previous Model state found.....')\n",
        "        \n",
        "        checkpoint = torch.load(saving_model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        steps_completed = checkpoint['steps_completed']\n",
        "        last_epoch_loss = checkpoint['last_epoch_loss']\n",
        "        \n",
        "        print('.....Previous Model and optimizer states has been retrieved.....')\n",
        "        print('{} steps completed'.format(steps_completed))\n",
        "        print('Last epoch cycle loss : {}'.format(last_epoch_loss))\n",
        "\n",
        "      else:\n",
        "        print('.....Previous Model state not found !!!.....')\n",
        "\n",
        "      save_model = True # After the finish of the program, it should save the model\n",
        "    \n",
        "    # Creating transform to apply on training dataset\n",
        "    training_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(270),\n",
        "                                         MeanSubtraction(),\n",
        "                                         ToTensor()])\n",
        "    \n",
        "    # Creating transform to apply on validation dataset\n",
        "    validation_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(270),\n",
        "                                         ToTensor()])\n",
        "    \n",
        "    # Creating training and validation dataset instance\n",
        "    training_dataset = KittiDataset(labels_dir=labels_dir, images_dir=images_dir,number_of_classes=classes, transform=training_dataset_transform)\n",
        "    validation_dataset = KittiDataset(labels_dir=validation_labels_dir, images_dir=validation_images_dir, number_of_classes=classes, transform=validation_dataset_transform)\n",
        "   \n",
        "    training_dataloader = DataLoader(dataset=training_dataset, batch_size=training_batch_size, shuffle=True, drop_last=True)\n",
        "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=validation_batch_size)\n",
        "    \n",
        "    print('..........Training and Validation Dataloader initialized..........')\n",
        "    \n",
        "    print('.....Training is starting.....')\n",
        "    \n",
        "    for epoch in range(0,epochs):\n",
        "      print('Training dataset for epoch : {}'.format(epoch))\n",
        "      \n",
        "      training_loss = train(model, optimizer, training_dataloader, training_batch_size, grids, grids_values, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "      \n",
        "      print('Loss for epoch :{} is {}'.format(epoch,training_loss))\n",
        "\n",
        "      # Logging training loss for hyper parameter tuning\n",
        "      if(logging):\n",
        "        experiment.log_metric(\"Training Loss\", training_loss )\n",
        "      \n",
        "    print('.....Validation is starting.....')\n",
        "    validation(model, validation_dataloader, validation_batch_size, object_detected_threshold, box_confidence_threshold, bounding_boxes, grids, grids_values)\n",
        "    #experiment.log_metric(\"Validation accuracy\", accuracy.item())\n",
        "   \n",
        "    steps_completed += int(((number_of_training_data)/training_batch_size)*epochs)\n",
        "    if (save_model):\n",
        "        torch.save({\n",
        "          'steps_completed': steps_completed,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'last_epoch_loss' : training_loss}, saving_model_path)\n",
        "          \n",
        "\n",
        "    print('..........Model is now trained over : {} steps..........'.format(steps_completed))        \n",
        "    print('..........Last epoch cycle loss : {}'.format(last_epoch_loss))\n",
        "    print('..........Current cycle last epoch loss : {}'.format(training_loss))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}