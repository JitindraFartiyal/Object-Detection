{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New Yolov1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JitindraFartiyal/Object-Detection/blob/object-detection-v1/Yolov1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzYgLQahHNYX",
        "colab_type": "text"
      },
      "source": [
        "Connecting to Google drive to upload dataset. This step is only required if you are using Google Colab and uploading dataset from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y6KmakpRJu0",
        "colab_type": "code",
        "outputId": "04a1b01a-e40d-43a2-f939-a7dbb422f532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCivLdScKWHU",
        "colab_type": "text"
      },
      "source": [
        "Importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxCUs78JHTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from comet_ml import Experiment\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from collections import OrderedDict \n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, datasets, utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHecGo64HXlm",
        "colab_type": "text"
      },
      "source": [
        "We need to convert class ['Car','Cyclist'....] in the label file into an integer. As, we are not using label file into our model, we need not to use one hot encoding or other encoding techniques. We are simply converting it for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyzx2ROpIMcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_encoding(label):\n",
        "   \n",
        "  for i in range(label.shape[0]):\n",
        "\n",
        "    if label.iloc[i,0] == 'Car':\n",
        "      label.iloc[i,0] = 0\n",
        "    elif label.iloc[i,0] == 'Cyclist':\n",
        "      label.iloc[i,0] = 1\n",
        "    elif label.iloc[i,0] == 'Pedestrian':\n",
        "      label.iloc[i,0] = 2\n",
        "    elif label.iloc[i,0] == 'Tram':\n",
        "      label.iloc[i,0] = 3\n",
        "    elif label.iloc[i,0] == 'Truck':\n",
        "      label.iloc[i,0] = 4\n",
        "    elif label.iloc[i,0] == 'Van':\n",
        "      label.iloc[i,0] = 5\n",
        "    elif label.iloc[i, 0] == 'DontCare':\n",
        "      label.iloc[i, 0] = 6\n",
        "    elif label.iloc[i,0] == 'Misc':\n",
        "      label.iloc[i,0] = 7\n",
        "    elif label.iloc[i,0] == 'Person_sitting':\n",
        "      label.iloc[i,0] = 8\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaCrcuTHIWGC",
        "colab_type": "text"
      },
      "source": [
        "Kitti Dataset has a different format for label file as compared to the YOLO format for label file. We need to convert format of our Kitti Dataset label file into format of YOLO label file.                                                       \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Note : We are rescaling coordinates of our bounding box into output image size and not the input image size as we need to compare the labels with the ouput of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onG5FIbgIeYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_label(label, number_of_classes, image, input_image_size):\n",
        "  \n",
        "  # In case of Bounding boxes, coordinate system doesnot start from bottom-left as we see normally in our mathematics, instead it starts from top-left corner\n",
        "  top_left_x = label[:,1]\n",
        "  top_left_y = label[:,2]\n",
        "  bottom_right_x = label[:,3]\n",
        "  bottom_right_y = label[:,4]\n",
        "\n",
        "  height = bottom_right_y - top_left_y\n",
        "  width = bottom_right_x - top_left_x\n",
        "  center_x = top_left_x + width/2\n",
        "  center_y = top_left_y + height/2\n",
        "\n",
        "  # Reducing the scale of the coordinates of bounding box in the label file into output image scale.\n",
        "  # We need to do this, so that at training and testing, we can compute loss easily, if all are in the same scale. \n",
        "  label[:,1] = (center_x/image.shape[1])*input_image_size\n",
        "  label[:,2] = (center_y/image.shape[0])*input_image_size\n",
        "  label[:,3] = (height /image.shape[0])*input_image_size\n",
        "  label[:,4] = (width/image.shape[1])*input_image_size\n",
        "\n",
        "  # Adding classes probabilites columns\n",
        "  target = np.zeros((label.shape[0],label.shape[1] + number_of_classes)) \n",
        "  target[:,0:5] = label\n",
        "  \n",
        "  for i in range(0,label.shape[0]):\n",
        "    if(target[i,0:1] == 0): # Prob_Class(Car) = 1 and rest 0, if Car is detected \n",
        "      target[i,5:6] = 1\n",
        "    elif(target[i,0:1] == 1): # Prob_Class(Cyclist) = 1 and rest 0, if Cyclist is detected\n",
        "      target[i,6:7] = 1\n",
        "    elif(target[i,0:1] == 2): # Prob_Class(Pedestrian) = 1 and rest 0, if Pedestrian is detected\n",
        "      target[i,7:8] = 1\n",
        "    elif(target[i,0:1] == 3): # Prob_Class(Tram) = 1 and rest 0, if Tram is detected\n",
        "      target[i,8:9] = 1\n",
        "    elif(target[i,0:1] == 4): # Prob_Class(Truck) = 1 and rest 0, if Truck is detected \n",
        "      target[i,9:10] = 1\n",
        "    elif(target[i,0:1] == 5): # Prob_Class(Van) = 1 and rest 0, if Van is detected \n",
        "      target[i,10:11] = 1\n",
        "\n",
        "  return target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xShT0UeFkCyi",
        "colab_type": "text"
      },
      "source": [
        "We need to preprocess the data. It means to keep the data i.e the input to our Convolutional Neural Network (CNN model) into an uniform form.\n",
        "Here, our input will be a dictionary of image and its label. Only, the images will be the input for our CNN model and label will be used for calculating loss. Largely, we only need to preprocess the data which we input in our CNN model with resizing, normalizing, mean subtraction etc.\n",
        "We also need to do padding on our label files, so that they become of same size tensor for collating in batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW-2inMnkFQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Resize(object):\n",
        "\n",
        "  def __init__(self, input_image_size):\n",
        "    # Input image size is the size of the image that we are putting it into our CNN Model. In this case, it is [270 X 270]\n",
        "    self.input_image_size = (input_image_size, input_image_size)\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = transform.resize(image, self.input_image_size, preserve_range=True, anti_aliasing=True)\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "class ToTensor(object):\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    image = image.transpose((2, 0, 1)) # Converting the image form from (H X W X C) into (C X H X W)\n",
        "\n",
        "    # If we donot use float() at end, by default, torch.from_numpy() will convert our input of our CNN model into a Float64 type\n",
        "    # We have to convert our CNN model type also in Float64 i.e Double or else it will throw error. \n",
        "    # By default, CNN model type is Float16, so better to convert the input into Float16 type here only\n",
        "    return {'image' : torch.from_numpy(image).float(),\n",
        "            'label' : torch.from_numpy(label).float()\n",
        "            }\n",
        "\n",
        "class Normalization(object):\n",
        "\n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    \n",
        "    image_mean = np.mean(image, axis = 0)\n",
        "    image_std = np.std(image, axis = 0)\n",
        "    image = (image_mean-image) / (image_std)\n",
        "\n",
        "    return {'image' : image, 'label' : label}\n",
        "\n",
        "# Our Dataset has different lengths data in our label file, so when stacking into a single batch during training, it throw error because of\n",
        "# variable dimensions. One of the solution is to pad the label file with an arbitrary number.  \n",
        "class BatchPadding(object):\n",
        "\n",
        "  def __init__(self, pad):\n",
        "    self.pad = pad\n",
        "  \n",
        "  def __call__(self, data_sample):\n",
        "    image, label = data_sample['image'], data_sample['label']\n",
        "    batched_label = np.zeros((self.pad,label.shape[1]))\n",
        "    batched_label[0:label.shape[0],:] = label\n",
        "\n",
        "    return {'image' : image, 'label' : batched_label}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpTW24lZIr0w",
        "colab_type": "text"
      },
      "source": [
        "Here, we are defining a class for our dataset. For our problem of Object Detection for Self Driving Cars, we are using KittiDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nlWbN5nIxZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KittiDataset(Dataset):\n",
        "\n",
        "    def __init__(self, labels_dir, images_dir, number_of_classes, input_image_size, transform=None):\n",
        "      self.labels_dir = labels_dir\n",
        "      self.images_dir = images_dir\n",
        "      self.number_of_classes = number_of_classes\n",
        "      self.input_image_size = input_image_size\n",
        "      self.transform = transform\n",
        "\n",
        "      self.labels_dict = {}\n",
        "      self.filename = []\n",
        "      self.__init__dataset()\n",
        "\n",
        "    def __init__dataset(self):\n",
        "      print('...............Initializing Dataset...............')\n",
        "      \n",
        "      index = 0\n",
        "      for file in os.listdir(self.labels_dir):\n",
        "        print('Reading label file : ' + file + '...')\n",
        "        \n",
        "        label_path = self.labels_dir + '/' + file\n",
        "        label = pd.read_csv(filepath_or_buffer=label_path, sep=' ', header=None, index_col=False)\n",
        "        \n",
        "        # Taking out relevant features out from the label dataframe\n",
        "        label = label.iloc[:,[0,4,5,6,7]] \n",
        "        label.columns = ['Class','TopLeftX','TopLeftY','BottomRightX','BottomRightY'] \n",
        "    \n",
        "        # Class Encoding\n",
        "        # Car=0, Cyclist=1, Pedestrian=2, Tram=3, Truck=4, Van=5.......\n",
        "        class_encoding(label)\n",
        "\n",
        "        self.labels_dict[index] = label\n",
        "        self.filename.append(file[0:6])\n",
        "        index = index + 1\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.labels_dict)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      image_path = self.images_dir + '/' + self.filename[index] + '.png'\n",
        "      image = io.imread(image_path)\n",
        "      \n",
        "      label = self.labels_dict[index]\n",
        "      label = label.to_numpy(dtype = np.float16) \n",
        "      \n",
        "      # Convert the label into YOLO format (class, center_x, center_y, height, width, class_prob1 ..... class_probn)\n",
        "      target = transform_label(label, self.number_of_classes, image, self.input_image_size)\n",
        "\n",
        "      data_sample = {'image' : image, 'label' : target}\n",
        "      \n",
        "      if self.transform:\n",
        "        data_sample = self.transform(data_sample)\n",
        "        \n",
        "      return data_sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvQc2zSI5QT",
        "colab_type": "text"
      },
      "source": [
        "After creating the datatset class, we now need to create our CNN model class, where we define our resnet34 architecture and our own custom Functional Layer. We also going to unfrezze the resnet50 layers as these are already having predetermined weights for classifying objects and we don't want to flush off them in our back propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm2SIYEI8TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_grids, number_of_cnn_output):\n",
        "\n",
        "      super(Net, self).__init__()\n",
        "      print('...............Initializing Convolutional Neural Network...............')\n",
        "      self.resnet50 = torchvision.models.resnet50(pretrained = True) # Using Resnet50 architecture\n",
        "      \n",
        "      # Freezing all the layers\n",
        "      self.resnet50.layer1.requires_grad=False\n",
        "      self.resnet50.layer2.requires_grad=False\n",
        "      self.resnet50.layer3.requires_grad=False\n",
        "      self.resnet50.layer4.requires_grad=False\n",
        "\n",
        "      # Adding new Fully Connected and Sigmoid layer\n",
        "      self.number_of_filters = self.resnet50.fc.out_features\n",
        "      self.input_grids = input_grids\n",
        "      self.number_of_cnn_output = number_of_cnn_output\n",
        "      \n",
        "      self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "      self.batch_norm_fc = nn.BatchNorm1d(num_features=self.number_of_filters)\n",
        "\n",
        "      self.fc1 = nn.Linear(self.number_of_filters, input_grids*number_of_cnn_output, bias=True)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.resnet50.conv1(x)\n",
        "      x = self.resnet50.bn1(x)\n",
        "      x = self.resnet50.relu(x)\n",
        "      x = self.resnet50.maxpool(x)\n",
        "\n",
        "      x = self.resnet50.layer1(x)\n",
        "      x = self.resnet50.layer2(x)\n",
        "      x = self.resnet50.layer3(x)\n",
        "      x = self.resnet50.layer4(x)\n",
        "      x = self.resnet50.avgpool(x)\n",
        "\n",
        "      x = x.view(-1,self.resnet50.fc.in_features)\n",
        "      x = self.resnet50.fc(x)\n",
        "\n",
        "      x = self.leaky_relu(x)\n",
        "      x = self.batch_norm_fc(x)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHSSYHVuJDIK",
        "colab_type": "text"
      },
      "source": [
        "After prediction, we will get many bounding boxes for a single class. To eliminate that, we need an algorithm to find which bounding box matches the ground truth bounding box by how much. We call this algorithm Intersection of Union (IOU).\n",
        "\n",
        "\n",
        "```\n",
        "IOU = (area of intersection) / (area of bounding box1 + area of bounding box2 - area of intersection) \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x-gK3JJJGq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_IOU(b1X, b1Y, b2X, b2Y, b3X, b3Y, b4X, b4Y):\n",
        "\n",
        "  # b1X, b1Y, b2X, b2Y corresponds to topleft and bottom right coordinates of bounding box1 \n",
        "  # b3X, b3Y, b4X, b4Y corresponds to topleft and bottom right coordinates of bounding box2\n",
        "  xA = max(b1X,b3X)\n",
        "  yA = max(b1Y,b3Y)\n",
        "  xB = min(b2X,b4X)\n",
        "  yB = min(b2Y,b4Y)\n",
        " \n",
        "  area_intersection = max(0,xB-xA+1) * max(0,yB-yA+1)\n",
        "  area_of_boundingbox1 = (b2X-b1X+1) * (b2Y-b1Y+1)\n",
        "  area_of_boundingbox2 = (b4X-b3X+1) * (b4Y-b3Y+1)\n",
        " \n",
        "  iou = area_intersection/float(area_of_boundingbox1 + area_of_boundingbox2 - area_intersection + 0.0001)\n",
        "  return iou\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAsc1wGM8urT",
        "colab_type": "text"
      },
      "source": [
        "After calculating IOU of all the bouding boxes, we need to return the bounding box whose IOU is the highest. \n",
        "\n",
        "\n",
        "---\n",
        "Remember the coordinates of the bounding box are scaled i.e x,y are offsets with respect to grid and h,w are scaled between 0 and 1 with respect to image height and width \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCijFzM48uBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, input_grids):\n",
        "  grid = np.sqrt(input_grids)\n",
        "  max_iou = 0\n",
        "  max_iou_index = 0\n",
        "  \n",
        "  x_offset = int(grid_cell/grid)\n",
        "  y_offset = int(grid_cell%grid)\n",
        "\n",
        "  topLeftX = x_offset * grid_offset\n",
        "  topLeftY = y_offset * grid_offset\n",
        "\n",
        "  for number_of_bbox in range(0,bounding_boxes):\n",
        "\n",
        "    predicted_center_x = (predicted_grid_output[(number_of_bbox*5) + 1].item() * grid_offset) + topLeftX\n",
        "    predicted_center_y = (predicted_grid_output[(number_of_bbox*5) + 2].item() * grid_offset) + topLeftY\n",
        "    predicted_height = predicted_grid_output[(number_of_bbox*5) + 3].item() * grid_offset * grid\n",
        "    predicted_width = predicted_grid_output[(number_of_bbox*5) + 4].item() * grid_offset * grid\n",
        "    \n",
        "    predicted_topLeftX = predicted_center_x - predicted_width/2\n",
        "    predicted_topLeftY = predicted_center_y - predicted_height/2 \n",
        "    predicted_bottomRightX = predicted_center_x + predicted_width/2\n",
        "    predicted_bottomRightY = predicted_center_y + predicted_height/2\n",
        "    \n",
        "    ground_truth_topLeftX = ground_truth_grid_output[1].item() - ground_truth_grid_output[4].item()/2\n",
        "    ground_truth_topLeftY = ground_truth_grid_output[2].item() - ground_truth_grid_output[3].item()/2\n",
        "    ground_truth_bottomRightX = ground_truth_grid_output[1].item() + ground_truth_grid_output[4].item()/2\n",
        "    ground_truth_bottomRightY = ground_truth_grid_output[2].item() + ground_truth_grid_output[3].item()/2\n",
        "\n",
        "    iou = calculate_IOU(predicted_topLeftX,predicted_topLeftY,predicted_bottomRightX,predicted_bottomRightY,\n",
        "                        ground_truth_topLeftX,ground_truth_topLeftY,ground_truth_bottomRightX,ground_truth_bottomRightY)\n",
        "\n",
        "    if(iou > max_iou):\n",
        "      max_iou = iou\n",
        "      max_iou_index = number_of_bbox\n",
        "\n",
        "  return max_iou_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZTDH6dJWlD",
        "colab_type": "text"
      },
      "source": [
        "Now, we will calculate the Loss function. It comprises of three losses :\n",
        "\n",
        "*   Classification Loss : if object is detected, the mean squared error loss of class probabilites\n",
        "*   Localization Loss : if object is detected, the mean squared error loss of coordinates of bounding box\n",
        "*   Confidence Loss : the mean squared error loss of box confidence, when object is detected and when it is not\n",
        "\n",
        "In the end, we will mulitply our loss with lambda_coord and lambda_noobject which regularize the imbalance and reduce the effect of background noise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h866UlMIy9la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def yolo_loss(batched_output, batched_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "  \n",
        "  total_loss = torch.tensor([0], dtype=torch.float)\n",
        "  grid = np.sqrt(input_grids)\n",
        "\n",
        "  for batch_number in range(batched_output.size()[0]):\n",
        "\n",
        "    classification_loss = torch.tensor([0], dtype=torch.float)\n",
        "    localization_loss_centerpoint = torch.tensor([0], dtype=torch.float)\n",
        "    localization_loss_aspect_ratio = torch.tensor([0], dtype=torch.float)\n",
        "    confidence_loss_object = torch.tensor([0], dtype=torch.float)\n",
        "    confidence_loss_noobject = torch.tensor([0], dtype=torch.float)\n",
        "\n",
        "    for grid_cell in range(batched_output.size()[1]):\n",
        "      \n",
        "      predicted_grid_output = batched_output[batch_number,grid_cell,:]\n",
        "     \n",
        "      # Logic to get the center coordinates of grid cell\n",
        "      x_offset = int(grid_cell / grid)\n",
        "      y_offset = int(grid_cell % grid)\n",
        "     \n",
        "      grid_cell_center_x = (x_offset*grid_offset) + (grid_offset/2)\n",
        "      grid_cell_center_y = (y_offset*grid_offset) + (grid_offset/2)\n",
        "     \n",
        "      object_present = -1\n",
        "      ground_truth_grid_output = torch.Tensor()\n",
        "       \n",
        "      for index in range(0,batched_label.size()[1]):\n",
        "        ground_truth_grid_output = batched_label[batch_number,index,:]\n",
        "        if (ground_truth_grid_output.sum() == 0):\n",
        "          break\n",
        "        \n",
        "        ground_truth_center_x = ground_truth_grid_output[1].item()\n",
        "        ground_truth_center_y = ground_truth_grid_output[2].item()\n",
        "\n",
        "        object_class = ground_truth_grid_output[0].item() # Stores which object is present in the grid cell which is responsible for detecting\n",
        "\n",
        "        # Finding whether grid detects an object or not\n",
        "        if(object_class >= 0 and object_class < classes and ground_truth_center_x < (grid_cell_center_x+(grid_offset/2)) and ground_truth_center_x >= (grid_cell_center_x-(grid_offset/2))\n",
        "            and ground_truth_center_y < (grid_cell_center_y+(grid_offset/2)) and ground_truth_center_y >= (grid_cell_center_y-(grid_offset/2))):\n",
        "          object_present = object_class\n",
        "          break\n",
        "            \n",
        "      # Calculating classification loss\n",
        "      if(object_present != -1):\n",
        "        partial_classification_loss = torch.tensor([0], dtype=torch.float)\n",
        "        \n",
        "        for target_class in range(classes):\n",
        "          if(object_class != object_present):\n",
        "            partial_classification_loss = partial_classification_loss + (predicted_grid_output[5*bounding_boxes+target_class]) ** 2 \n",
        "      \n",
        "        classification_loss = classification_loss + partial_classification_loss + (1 - predicted_grid_output[5*bounding_boxes + int(object_present)])**2\n",
        "\n",
        "        # Calculating which bounding box has highest IOU with ground truth bounding box\n",
        "        highest_iou_bbox_index = find_highest_IOU(predicted_grid_output, ground_truth_grid_output, bounding_boxes, grid_cell, grid_offset, input_grids)\n",
        "\n",
        "        # Calculating localization loss of center points and aspect ratio\n",
        "\n",
        "        ground_truth_height = (ground_truth_grid_output[3])/(grid * grid_offset)\n",
        "        ground_truth_width = (ground_truth_grid_output[4])/(grid * grid_offset)\n",
        "        ground_truth_center_x = ((ground_truth_grid_output[1]) % grid_offset)/grid_offset\n",
        "        ground_truth_center_y = ((ground_truth_grid_output[2]) % grid_offset)/grid_offset\n",
        "\n",
        "        localization_loss_centerpoint = localization_loss_centerpoint + ((predicted_grid_output[1]-ground_truth_center_x))**2 + ((predicted_grid_output[2]-ground_truth_center_y))**2\n",
        "        localization_loss_aspect_ratio = localization_loss_aspect_ratio + (torch.sqrt(predicted_grid_output[3])-torch.sqrt(ground_truth_height))**2 + (torch.sqrt(predicted_grid_output[4])-torch.sqrt(ground_truth_width))**2\n",
        "        \n",
        "        # Calculating Confidence loss, if object detected\n",
        "        confidence_loss_object = confidence_loss_object + (1 - predicted_grid_output[highest_iou_bbox_index*5])**2\n",
        "              \n",
        "      # Calculating Confidence loss, if object not detected\n",
        "      else:\n",
        "        for number_of_bounding_box in range(bounding_boxes):\n",
        "          confidence_loss_noobject = confidence_loss_noobject + (predicted_grid_output[number_of_bounding_box*5])**2\n",
        "\n",
        "    total_loss = total_loss + classification_loss + lambda_coord*localization_loss_centerpoint + lambda_coord*localization_loss_aspect_ratio + confidence_loss_object + lambda_noobject*confidence_loss_noobject \n",
        "\n",
        "  batch_loss = total_loss/batched_output.size()[0] \n",
        "  \n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9vSE7fmJeRR",
        "colab_type": "text"
      },
      "source": [
        "Our Convolutional Neural newtork is defined, dataset is defined, loss function is defined. Now, we will train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbFltpQ6JgdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, scheduler, training_dataloader, training_batch_size, input_grids, grid_offset, number_of_cnn_output, bounding_boxes, classes, lambda_coord, lambda_noobject):\n",
        "  \n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training \n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode\n",
        "  model.train()\n",
        "  batch_loss = 0\n",
        "\n",
        "  for batch_index, batched_sample in enumerate(training_dataloader):\n",
        "\n",
        "    batched_image = torch.tensor(batched_sample['image'], requires_grad=True, dtype=torch.float)\n",
        "    batched_label = torch.tensor(batched_sample['label'], requires_grad=True, dtype=torch.float)\n",
        "    batched_output = model(batched_image)\n",
        "    batched_output = batched_output.view(training_batch_size, input_grids, number_of_cnn_output) # Convert the output size into [N X GRIDS X (5 * B + C)]\n",
        "\n",
        "    loss = yolo_loss(batched_output, batched_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "    print('Training Loss for batch_index : {} is {}'.format(batch_index,loss))\n",
        "    batch_loss = batch_loss + loss.item()\n",
        "   \n",
        "    optimizer.zero_grad()    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  return batch_loss/len(training_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7NTe2l9Byp",
        "colab_type": "text"
      },
      "source": [
        "Non Max Supression algorithm to remove duplicate bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYtmeEv5Q2PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def non_max_suppression(bbox_list, iou_threshold):\n",
        "\n",
        "  sorted_bbox_list = OrderedDict()\n",
        "  for key in sorted(bbox_list, reverse=True):\n",
        "    sorted_bbox_list[key] = bbox_list[key]\n",
        "  \n",
        "  deleted_elements_list = []\n",
        "  for key1 in sorted_bbox_list:\n",
        "    for key2 in sorted_bbox_list:\n",
        "\n",
        "      if(key1 != key2):\n",
        "        bbox1 = sorted_bbox_list[key1]\n",
        "        bbox2 = sorted_bbox_list[key2]\n",
        "        iou = calculate_IOU(bbox1['tlx'], bbox1['tly'],bbox1['brx'],bbox1['bry'],bbox2['tlx'],bbox2['tly'],bbox2['brx'],bbox2['bry'])\n",
        "\n",
        "        if(iou >= iou_threshold):\n",
        "          if key2 not in deleted_elements_list:\n",
        "            deleted_elements_list.append(key2)\n",
        "\n",
        "  for del_ele in deleted_elements_list:\n",
        "    del sorted_bbox_list[del_ele]\n",
        "  \n",
        "  return sorted_bbox_list  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOqC1ERJ5xp",
        "colab_type": "text"
      },
      "source": [
        "We have trained our model, now we will validate our model. Validation is required to tune our model parameters and hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA1zdHgdJ8Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model, validation_dataloader, validation_batch_size, classes, input_grids, grid_offset, number_of_cnn_output, saving_results_path, bounding_boxes, validation_images_dir, lambda_coord, lambda_noobject, object_detected_threshold, box_confidence_threshold, iou_threshold):\n",
        "\n",
        "  # This is inbuilt function of Pytorch and it is important to call it in training \n",
        "  # as few function like dropout and batch norm works differently in training mode than in evaluation mode  \n",
        "\n",
        "  model.eval() \n",
        "  batch_loss = 0\n",
        "  filename=[]\n",
        "  image_index = 0\n",
        "  grid = np.sqrt(input_grids)\n",
        "\n",
        "  for file in os.listdir(validation_images_dir):\n",
        "    filename.append(file[0:6])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_index, batched_validation_sample in enumerate(validation_dataloader):\n",
        "      validation_image = batched_validation_sample['image']\n",
        "      validation_label = batched_validation_sample['label']\n",
        "      output = model(validation_image)\n",
        "      output = output.view(validation_batch_size, input_grids, number_of_cnn_output)\n",
        "\n",
        "      image_path = validation_images_dir + '/' + filename[image_index] + '.png'\n",
        "      original_image = io.imread(image_path)\n",
        "      \n",
        "      loss = yolo_loss(output, validation_label, input_grids, grid_offset, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "      print('Validation Loss for batch_index : {} is {}'.format(batch_index,loss))\n",
        "      batch_loss += loss.item()     \n",
        "      \n",
        "      for batch_number in range(validation_batch_size):\n",
        "        bbox_list = OrderedDict()\n",
        "        for grid_cell in range(input_grids):\n",
        "          grid_output = output[batch_number,grid_cell,:]\n",
        "          \n",
        "          # Logic to get the center, top-left and bottom-right coordinates of grid cell\n",
        "          x_offset = int(grid_cell / grid)\n",
        "          y_offset = int(grid_cell % grid)\n",
        "\n",
        "          grid_cell_topleftX = x_offset * grid_offset \n",
        "          grid_cell_topleftY = y_offset * grid_offset\n",
        "\n",
        "          for number_of_bounding_box in range(bounding_boxes):\n",
        "            if(grid_output[number_of_bounding_box*5] > object_detected_threshold):\n",
        "\n",
        "              class_probabilities = grid_output[5*bounding_boxes:]\n",
        "              max_class_probabilites, index = torch.max(class_probabilities,0)\n",
        "              \n",
        "              center_x = grid_output[(number_of_bounding_box*5)+1] * grid_offset + grid_cell_topleftX\n",
        "              center_y = grid_output[(number_of_bounding_box*5)+2] * grid_offset + grid_cell_topleftY\n",
        "              height = grid_output[(number_of_bounding_box*5)+3] * grid * grid_offset\n",
        "              width = grid_output[(number_of_bounding_box*5)+4] * grid * grid_offset\n",
        "\n",
        "              top_left_x = int((((center_x - (width/2))/(grid*grid_offset))*original_image.shape[1]).item())\n",
        "              top_left_y = int((((center_y - (height/2))/(grid*grid_offset))*original_image.shape[0]).item())\n",
        "              bottom_right_x = int((((center_x + (width/2))/(grid*grid_offset))*original_image.shape[1]).item())\n",
        "              bottom_right_y = int((((center_y + (height/2))/(grid*grid_offset))*original_image.shape[0]).item())\n",
        "              \n",
        "              predicted_strength = max_class_probabilites.item() * grid_output[number_of_bounding_box*5].item()\n",
        "              accuracy = round(predicted_strength * 100,2)\n",
        "\n",
        "              if(top_left_x >= 0 and top_left_y >= 0 and bottom_right_x >= 0 and bottom_right_y >= 0 and predicted_strength >= box_confidence_threshold):\n",
        "                bbox_list[predicted_strength]={'tlx':top_left_x,'tly':top_left_y,'brx':bottom_right_x,'bry':bottom_right_y,'accuracy':accuracy,'index':index.item()}\n",
        "        \n",
        "        updated_bbox_list = non_max_suppression(bbox_list,iou_threshold)      \n",
        "        name = 'Unknown'\n",
        "        color = (130,130,130)\n",
        "        \n",
        "        for key in updated_bbox_list:\n",
        "          bbox = updated_bbox_list[key]\n",
        "          object_class = bbox['index']\n",
        "          \n",
        "          top_left_x = bbox['tlx']\n",
        "          top_left_y = bbox['tly']\n",
        "          bottom_right_x = bbox['brx']\n",
        "          bottom_right_y = bbox['bry']\n",
        "        \n",
        "          if(object_class == 0):\n",
        "            name = 'Car'\n",
        "            color = (255,255,255)\n",
        "          elif(object_class == 1):\n",
        "            name = 'Cyclist'\n",
        "            color = (0,0,255)\n",
        "          elif(object_class == 2):\n",
        "            name = 'Pedestrian'\n",
        "            color = (0,255,0)\n",
        "          elif(object_class == 3):\n",
        "            name = 'Tram'\n",
        "            color = (255,0,0)\n",
        "          elif(object_class == 4):\n",
        "            name = 'Truck'\n",
        "            color = (0,255,255)\n",
        "          elif(object_class == 5):\n",
        "            name = 'Van' \n",
        "            color = (255,0,255)\n",
        "\n",
        "          name = name + '(' + str(bbox['accuracy']) + ')'               \n",
        "                \n",
        "          original_image = cv2.rectangle(original_image,(top_left_x,top_left_y),(bottom_right_x,bottom_right_y),color,2)\n",
        "          original_image = cv2.rectangle(original_image,(top_left_x,top_left_y-30),(top_left_x+125,top_left_y),color,cv2.FILLED)\n",
        "          cv2.putText(original_image, name, (top_left_x, top_left_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "        \n",
        "        save_image_path = saving_results_path + str(batch_index) + '.png'\n",
        "        file_saved = cv2.imwrite(save_image_path, original_image)\n",
        "\n",
        "      image_index +=1\n",
        "\n",
        "  return batch_loss/len(validation_dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9gLfZyJox3",
        "colab_type": "text"
      },
      "source": [
        "We will finish our program by writing a main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkCwlPQJqWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    print('...............Main Function starts...............')\n",
        "    \n",
        "    # Training Settings\n",
        "    base_lr = 0.000001 # Hyper parameters\n",
        "    max_lr = 0.001 # Hyper parameters\n",
        "    momentum = 0.9 # Hyper parameters\n",
        "    epochs = 100  # Hyper parameters\n",
        "    training_batch_size = 2 # Hyper parameters\n",
        "    validation_batch_size = 1\n",
        "    \n",
        "    object_detected_threshold = 0.25 # Model Parameters\n",
        "    box_confidence_threshold = 0.5 # Model Parameters\n",
        "    iou_threshold = 0.4 # Model Parameters\n",
        "\n",
        "    input_image_size = 224 # Model Parameters\n",
        "    input_grids = 7*7 # Model Parameters\n",
        "    grid_offset = input_image_size/np.sqrt(input_grids) # Model Parameters\n",
        "    bounding_boxes = 2 # Model Parameters\n",
        "    \n",
        "    classes = 6 # Model Parameters\n",
        "    number_of_cnn_output = (5*bounding_boxes) + classes # Model Parameters\n",
        "    lambda_coord = 5 # Model Parameters\n",
        "    lambda_noobject = 0.5 # Model Parameters\n",
        "    \n",
        "    save_model = False\n",
        "    seed = 1\n",
        "    logging = True\n",
        "    steps_completed = 0\n",
        "    last_epoch_loss = 0\n",
        "    number_of_training_data = 0\n",
        "    training_loss = 0\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Comet ML Settings for visualizing loss function and hyper parameters\n",
        "    if(logging):\n",
        "      experiment = Experiment(api_key=\"Vxlozksi1tLwXJlmZYjfQVm7w\", project_name=\"object-detection\", workspace=\"jayfartiyal\")\n",
        "      hyper_parameters = {\"lr\": base_lr, \"epochs\": epochs, \"batch_size\":training_batch_size} \n",
        "      experiment.log_parameters(hyper_parameters)\n",
        "\n",
        "    # Images and labels Directory\n",
        "    training_labels_dir = r'/content/gdrive/My Drive/kitti_single_nano/training/label_2'\n",
        "    training_images_dir = r'/content/gdrive/My Drive/kitti_single_nano/training/image_2'\n",
        "    validation_images_dir = r'/content/gdrive/My Drive/kitti_single_nano/validation/image_2'\n",
        "    validation_labels_dir = r'/content/gdrive/My Drive/kitti_single_nano/validation/label_2'\n",
        "    saving_model_path = r'/content/gdrive/My Drive/kitti_single_nano/validation/resnet50_class6_sgd_clr_version1.cnn.pt'\n",
        "    saving_results_path = r'/content/gdrive/My Drive/kitti_single_nano/validation/results/image'\n",
        "\n",
        "    if(save_model == False):\n",
        "      #Inititalizing model and optimizer\n",
        "      model = Net(input_grids, number_of_cnn_output)\n",
        "      optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=momentum)\n",
        "      scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer,base_lr=base_lr,max_lr=max_lr,step_size_up=250)\n",
        "\n",
        "      print('...............Convolutional Neural Network model and optimizer has been initialized...............')\n",
        "\n",
        "      # Retrieving model and optimizer states if present \n",
        "      if(os.path.isfile(saving_model_path)):\n",
        "        print('Previous Model state found...............')\n",
        "        \n",
        "        checkpoint = torch.load(saving_model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        steps_completed = checkpoint['steps_completed']\n",
        "        last_epoch_loss = checkpoint['last_epoch_loss']\n",
        "        \n",
        "        print('Previous Model and optimizer states has been retrieved...............')\n",
        "        print('{} steps completed..........'.format(steps_completed))\n",
        "        print('Last epoch cycle loss : {}..........'.format(last_epoch_loss))          \n",
        "      else:\n",
        "        print('Previous Model state not found !!!...............')\n",
        "\n",
        "      save_model = True # After the finish of the program, it should save the model\n",
        "    \n",
        "    # Creating transform to apply on training dataset\n",
        "    training_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(input_image_size),\n",
        "                                         Normalization(),\n",
        "                                         ToTensor()])\n",
        "    \n",
        "    # Creating transform to apply on validation dataset\n",
        "    validation_dataset_transform = transforms.Compose([\n",
        "                                         BatchPadding(100),\n",
        "                                         Resize(input_image_size),\n",
        "                                         Normalization(),\n",
        "                                         ToTensor()])\n",
        "    \n",
        "    # Creating training and validation dataset instance\n",
        "    training_dataset = KittiDataset(labels_dir=training_labels_dir, images_dir=training_images_dir, number_of_classes=classes, input_image_size=input_image_size, transform=training_dataset_transform)\n",
        "    validation_dataset = KittiDataset(labels_dir=validation_labels_dir, images_dir=validation_images_dir, number_of_classes=classes, input_image_size=input_image_size, transform=validation_dataset_transform)\n",
        "    number_of_training_data = training_dataset.__len__()\n",
        "\n",
        "    training_dataloader = DataLoader(dataset=training_dataset, batch_size=training_batch_size, shuffle=True, drop_last=True)\n",
        "    validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=validation_batch_size)\n",
        "    \n",
        "    print('...............Training and Validation Dataloader initialized...............')\n",
        "    print('...............Training is starting...............')\n",
        "    \n",
        "    for epoch in range(0,epochs):\n",
        "      training_loss = train(model, optimizer, scheduler, training_dataloader, training_batch_size, input_grids, grid_offset, number_of_cnn_output, bounding_boxes, classes, lambda_coord, lambda_noobject)\n",
        "      print('Training Loss for epoch :{} is {}'.format(epoch,training_loss))\n",
        "      \n",
        "      # Logging training loss for hyper parameter tuning\n",
        "      if(logging):\n",
        "        experiment.log_metric(\"Training Loss\", training_loss)\n",
        "      \n",
        "      print('................Validation is starting.................')\n",
        "      validation_loss = validation(model, validation_dataloader, validation_batch_size, classes, input_grids, grid_offset, number_of_cnn_output, saving_results_path, bounding_boxes, validation_images_dir, lambda_coord, lambda_noobject, object_detected_threshold, box_confidence_threshold, iou_threshold)\n",
        "      print('Validation Loss for epoch :{} is {}'.format(epoch,validation_loss))\n",
        "\n",
        "      # Logging training loss for hyper parameter tuning\n",
        "      if(logging):\n",
        "        experiment.log_metric(\"Validation Loss\", validation_loss)\n",
        "\n",
        "    steps_completed += int(((number_of_training_data)/training_batch_size)*epochs)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save({\n",
        "          'steps_completed': steps_completed,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'last_epoch_loss' : training_loss}, saving_model_path)\n",
        "        print('..............Convolutional Neural Network Model parameters are saved...............')\n",
        "          \n",
        "\n",
        "    print('...............Model is now trained over : {} steps...............'.format(steps_completed))        \n",
        "    print('...............Last epoch cycle loss : {} ...............'.format(last_epoch_loss))\n",
        "    print('...............Current cycle last epoch loss : {} ...............'.format(training_loss))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}